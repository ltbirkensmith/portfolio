<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Autonomous Self-Landing UAV - Liam Smith</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
            line-height: 1.6;
            color: #000;
            background: #fff;
        }
        
        .project-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 60px 20px;
        }
        
        .back-link {
            display: inline-block;
            color: #9BCBEB;
            text-decoration: none;
            margin-bottom: 30px;
            font-size: 16px;
        }
        
        .back-link:hover {
            color: #6BA3C3;
        }
        
        .project-header {
            margin-bottom: 40px;
        }
        
        .project-title {
            font-size: 42px;
            font-weight: 700;
            margin-bottom: 20px;
        }
        
        .project-subtitle {
            font-size: 20px;
            color: #666;
            margin-bottom: 30px;
        }
        
        .project-meta {
            display: flex;
            gap: 30px;
            flex-wrap: wrap;
            margin-bottom: 40px;
        }
        
        .meta-item {
            display: flex;
            flex-direction: column;
        }
        
        .meta-label {
            font-size: 12px;
            text-transform: uppercase;
            color: #666;
            margin-bottom: 5px;
            letter-spacing: 1px;
        }
        
        .meta-value {
            font-size: 16px;
            color: #000;
        }
        
        .content-section {
            margin-bottom: 40px;
        }
        
        .section-title {
            font-size: 28px;
            font-weight: 600;
            margin-bottom: 20px;
            color: #000;
        }
        
        .section-text {
            font-size: 18px;
            line-height: 1.8;
            color: #333;
            margin-bottom: 20px;
        }
        
        .tech-tags {
            display: flex;
            flex-wrap: wrap;
            gap: 10px;
            margin-bottom: 30px;
        }
        
        .tech-tag {
            background: #9BCBEB;
            color: #fff;
            padding: 8px 16px;
            border-radius: 20px;
            font-size: 14px;
        }
        
        .achievements-list {
            list-style: none;
            margin-bottom: 30px;
        }
        
        .achievements-list li {
            padding-left: 30px;
            position: relative;
            margin-bottom: 15px;
            font-size: 18px;
            line-height: 1.6;
        }
        
        .achievements-list li:before {
            content: "→";
            position: absolute;
            left: 0;
            color: #9BCBEB;
            font-weight: bold;
        }
        
        .status-badge {
            display: inline-block;
            background: #fff3cd;
            color: #856404;
            padding: 8px 16px;
            border-radius: 20px;
            font-size: 14px;
            font-weight: 600;
            margin-bottom: 20px;
        }
        
        @media (max-width: 768px) {
            .project-title {
                font-size: 32px;
            }
            
            .project-subtitle {
                font-size: 18px;
            }
            
            .section-title {
                font-size: 24px;
            }
            
            .section-text {
                font-size: 16px;
            }
        }
    </style>
</head>
<body>
    <div class="project-container">
        <a href="index.html" class="back-link">← Back to Portfolio</a>
        
        <div class="project-header">
            <h1 class="project-title">Autonomous Self-Landing UAV</h1>
            <p class="project-subtitle">Vision-Based Autonomous Landing System Using YOLO and Raspberry Pi</p>
        </div>
        
        <div class="project-meta">
            <div class="meta-item">
                <span class="meta-label">Type</span>
                <span class="meta-value">Personal Project</span>
            </div>
            <div class="meta-item">
                <span class="meta-label">Started</span>
                <span class="meta-value">Summer 2025</span>
            </div>
            <div class="meta-item">
                <span class="meta-label">Expected First Flight</span>
                <span class="meta-value">Summer 2026</span>
            </div>
            <div class="meta-item">
                <span class="meta-label">Status</span>
                <span class="meta-value">Development & Testing</span>
            </div>
        </div>
        
        <span class="status-badge">⚙️ Project In Progress</span>
        
        <div class="content-section">
            <h2 class="section-title">Project Vision</h2>
            <p class="section-text">
                This summer, I set out to build a model airplane capable of landing itself autonomously using only visual guidance—no GPS, no external sensors, just computer vision and intelligent decision-making. The goal is to create a fully autonomous landing system that can identify a runway, calculate approach parameters, and execute a safe landing using real-time image processing and flight control.
            </p>
            <p class="section-text">
                This project represents the intersection of computer vision, machine learning, embedded systems, and aeronautical engineering. It's inspired by how human pilots land aircraft—primarily through visual cues—and aims to replicate that capability in an autonomous system that can adapt to varying conditions and make split-second decisions.
            </p>
        </div>
        
        <div class="content-section">
            <h2 class="section-title">Technology Stack</h2>
            <div class="tech-tags">
                <span class="tech-tag">YOLO (You Only Look Once)</span>
                <span class="tech-tag">Raspberry Pi</span>
                <span class="tech-tag">ESP32 Flight Controller</span>
                <span class="tech-tag">Arduino Backup System</span>
                <span class="tech-tag">Computer Vision</span>
                <span class="tech-tag">Real-Time Processing</span>
                <span class="tech-tag">Python</span>
                <span class="tech-tag">Machine Learning</span>
                <span class="tech-tag">Embedded Systems</span>
            </div>
        </div>
        
        <div class="content-section">
            <h2 class="section-title">System Architecture</h2>
            <p class="section-text">
                <strong>Vision System (Raspberry Pi + YOLO):</strong> The Raspberry Pi serves as the "eyes" and "brain" of the aircraft, running a customized YOLO visual recognition model trained to identify runways, calculate distances, and detect obstacles. The YOLO model processes video frames in real-time to extract critical landing information including runway orientation, distance, and approach angle.
            </p>
            <p class="section-text">
                <strong>Primary Flight Controller (ESP32):</strong> The ESP32 microcontroller acts as the primary flight controller, receiving commands from the Raspberry Pi and translating them into servo movements and throttle adjustments. It handles the real-time control loop necessary for stable flight, responding to vision system inputs within milliseconds.
            </p>
            <p class="section-text">
                <strong>Safety Backup (Arduino):</strong> An Arduino microcontroller serves as a hardcoded backup system that can take over control if the primary systems fail. It provides basic stabilization and emergency landing capabilities, ensuring the aircraft can be recovered even if the vision or ESP32 systems malfunction.
            </p>
        </div>
        
        <div class="content-section">
            <h2 class="section-title">Development Progress</h2>
            <ul class="achievements-list">
                <li>Programmed initial YOLO visual recognition model for runway detection and identification</li>
                <li>Integrated Raspberry Pi hardware with camera system for real-time image capture</li>
                <li>Developed real-time decision framework architecture for processing vision data</li>
                <li>Configured ESP32 as flight controller with servo and throttle control capabilities</li>
                <li>Implemented Arduino hardcode backup system for emergency scenarios</li>
                <li>Conducted testing with real runway images to validate accuracy and object detection</li>
                <li>Measured and optimizing response time for high-speed flight compatibility</li>
                <li>Created mockup integration of system components within test aircraft</li>
            </ul>
        </div>
        
        <div class="content-section">
            <h2 class="section-title">Current Challenges</h2>
            <p class="section-text">
                <strong>Processing Speed Optimization:</strong> The primary challenge is refining the YOLO model and decision framework to operate on a timescale workable for high-speed flight. Model airplanes approach and land at speeds of 20-40 mph, requiring processing and decision cycles measured in milliseconds. Current work focuses on model optimization, frame rate management, and predictive algorithms that can anticipate required control inputs.
            </p>
            <p class="section-text">
                <strong>Real-Time Decision Making:</strong> Translating vision data into flight control commands requires sophisticated decision algorithms that can handle varying lighting conditions, wind effects, and unexpected obstacles. The system must balance aggressive maneuvering for precise control with smooth, stable flight that prevents oscillations or loss of control.
            </p>
            <p class="section-text">
                <strong>Hardware Integration:</strong> Integrating multiple processors (Raspberry Pi, ESP32, Arduino) with sensors, servos, and power systems within the confined space of a model aircraft presents significant packaging challenges. Weight distribution, electromagnetic interference, and thermal management must all be carefully considered.
            </p>
        </div>
        
        <div class="content-section">
            <h2 class="section-title">YOLO Visual Recognition Model</h2>
            <p class="section-text">
                YOLO (You Only Look Once) is a state-of-the-art real-time object detection system. Unlike traditional computer vision approaches that require multiple passes over an image, YOLO analyzes the entire image in a single pass, making it ideal for applications requiring fast, real-time performance like autonomous flight.
            </p>
            <p class="section-text">
                <strong>Training Process:</strong> The model is being trained on a custom dataset of runway images captured from various angles, distances, and lighting conditions. Training includes normal runways, emergency landing strips, and various ground markings to ensure robust detection capability across different scenarios.
            </p>
            <p class="section-text">
                <strong>Output Data:</strong> The trained model outputs runway bounding boxes, confidence scores, distance estimates, and orientation angles. This data feeds into the decision framework which calculates required heading adjustments, descent rates, and throttle settings for a safe landing approach.
            </p>
        </div>
        
        <div class="content-section">
            <h2 class="section-title">Decision Framework Architecture</h2>
            <ul class="achievements-list">
                <li>State machine architecture tracking flight phases: search, approach, final, flare, touchdown</li>
                <li>Proportional-Integral-Derivative (PID) controllers for stable heading and altitude control</li>
                <li>Kalman filtering to smooth noisy vision data and predict future positions</li>
                <li>Risk assessment algorithms that evaluate landing safety and can trigger go-around</li>
                <li>Wind estimation using ground speed calculations and drift compensation</li>
                <li>Energy management ensuring sufficient altitude and airspeed throughout approach</li>
                <li>Failsafe logic monitoring system health and triggering backup systems if needed</li>
            </ul>
        </div>
        
        <div class="content-section">
            <h2 class="section-title">Testing Methodology</h2>
            <p class="section-text">
                <strong>Benchtop Testing:</strong> Currently testing the vision system and decision framework using pre-recorded runway footage and simulated flight data. This allows rapid iteration without risk to aircraft hardware. Accuracy and response time measurements inform optimization efforts.
            </p>
            <p class="section-text">
                <strong>Ground Testing:</strong> Next phase involves testing the complete integrated system on the ground, verifying communication between components, servo response times, and system power requirements. Ground tests validate the hardware integration before risking the aircraft in flight.
            </p>
            <p class="section-text">
                <strong>Flight Testing Plan:</strong> Initial flight tests will use manual takeoff with gradual handoff to autonomous control at safe altitude. Testing will progress through straight-and-level autonomous flight, then gentle turns and descents, before attempting full autonomous landing sequences. Each phase builds confidence in system reliability.
            </p>
        </div>
        
        <div class="content-section">
            <h2 class="section-title">Technical Innovations</h2>
            <p class="section-text">
                <strong>Lightweight Implementation:</strong> Achieving autonomous landing capability while maintaining acceptable aircraft weight and balance requires careful component selection and custom circuit integration. The entire system must weigh less than 500 grams to avoid significantly impacting flight characteristics.
            </p>
            <p class="section-text">
                <strong>Adaptive Algorithms:</strong> The decision framework includes adaptive elements that learn from each landing attempt, adjusting control gains and approach parameters based on observed performance. This allows the system to improve over time and adapt to different aircraft configurations.
            </p>
            <p class="section-text">
                <strong>Redundancy Architecture:</strong> Triple-redundant control system (Raspberry Pi, ESP32, Arduino) provides multiple levels of safety. Each system can operate independently if higher-level systems fail, ensuring the aircraft can always be recovered.
            </p>
        </div>
        
        <div class="content-section">
            <h2 class="section-title">Skills Being Developed</h2>
            <ul class="achievements-list">
                <li>Machine learning model training and optimization for embedded systems</li>
                <li>Real-time computer vision programming and image processing</li>
                <li>Embedded systems integration with multiple microcontrollers</li>
                <li>Flight control algorithms and PID tuning</li>
                <li>Sensor fusion and state estimation techniques</li>
                <li>System architecture design for safety-critical applications</li>
                <li>Aerodynamic considerations for autonomous flight</li>
                <li>Systematic testing and debugging of complex integrated systems</li>
            </ul>
        </div>
        
        <div class="content-section">
            <h2 class="section-title">Timeline & Milestones</h2>
            <p class="section-text">
                <strong>Summer 2025:</strong> Project initiation, YOLO model development, hardware procurement and initial integration.
            </p>
            <p class="section-text">
                <strong>Fall 2025 - Spring 2026:</strong> Model training and optimization, benchtop testing, decision framework refinement, hardware integration completion.
            </p>
            <p class="section-text">
                <strong>Summer 2026:</strong> Ground testing, initial flight tests with manual control, progressive autonomous capability testing, first attempted autonomous landing.
            </p>
            <p class="section-text">
                <strong>Expected Achievement:</strong> Successful demonstration of fully autonomous landing capability using only visual guidance by Summer 2026.
            </p>
        </div>
        
        <div class="content-section">
            <h2 class="section-title">Future Enhancements</h2>
            <ul class="achievements-list">
                <li>Obstacle detection and avoidance during approach</li>
                <li>Crosswind landing capability with automatic crab angle correction</li>
                <li>Night landing using visible runway lighting</li>
                <li>Emergency landing site selection when runway unavailable</li>
                <li>Full autonomous flight including takeoff, navigation, and landing</li>
                <li>Multiple aircraft coordination for sequential landings</li>
            </ul>
        </div>
        
        <div class="content-section">
            <h2 class="section-title">Broader Applications</h2>
            <p class="section-text">
                While this project focuses on model aircraft, the technologies and techniques being developed have broader applications in autonomous vehicles, delivery drones, emergency response UAVs, and agricultural automation. Vision-based landing systems are particularly valuable in GPS-denied environments or when pinpoint accuracy is required beyond GPS capabilities.
            </p>
        </div>
    </div>
</body>
</html>
